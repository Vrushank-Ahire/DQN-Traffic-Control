{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5oLNTmDVjV4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import re\n",
        "from collections import namedtuple, deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Constants\n",
        "BUFFER_SIZE = int(1e5)\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "TAU = 1e-3\n",
        "LR = 5e-4\n",
        "UPDATE_EVERY = 4\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "dI-gWVlHVkip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        self.t_step = 0\n",
        "\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, state, eps=None):\n",
        "        eps = eps if eps is not None else self.epsilon\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
      ],
      "metadata": {
        "id": "YKdx0h4jVnTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "o3Rq3D6pVqDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrafficEnv:\n",
        "    def __init__(self, data, max_steps=1000):\n",
        "        self.data = data\n",
        "        self.max_steps = max_steps\n",
        "        self.current_step = 0\n",
        "        self.num_samples = len(self.data)\n",
        "        self.traffic_light_state = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.state = self._get_state(self.current_step)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        self._update_traffic_light(action)\n",
        "        reward = self._calculate_reward()\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps or self.current_step >= self.num_samples\n",
        "\n",
        "        if not done:\n",
        "            next_state = self._get_state(self.current_step)\n",
        "            return next_state, reward, done\n",
        "        else:\n",
        "            return self.state, reward, done\n",
        "\n",
        "    def _get_state(self, step):\n",
        "        if step < self.num_samples:\n",
        "            row = self.data.iloc[step]\n",
        "            vehicle_volume = self._parse_vehicle_volume(row)\n",
        "            return np.array(vehicle_volume + [self.traffic_light_state])\n",
        "        else:\n",
        "            # Return the last known state if we're beyond the data\n",
        "            return self.state\n",
        "\n",
        "    def _parse_vehicle_volume(self, row):\n",
        "        volume_data = row['Vehicle Volume By Each Direction of Traffic']\n",
        "        volumes = {'East Bound': 0, 'West Bound': 0, 'North Bound': 0, 'South Bound': 0}\n",
        "\n",
        "        matches = re.findall(r'(\\w+\\s*\\w*)\\s*:\\s*(\\d+)', volume_data)\n",
        "\n",
        "        for direction, volume in matches:\n",
        "            direction = direction.strip()\n",
        "            if 'East' in direction:\n",
        "                volumes['East Bound'] = int(volume)\n",
        "            elif 'West' in direction:\n",
        "                volumes['West Bound'] = int(volume)\n",
        "            elif 'North' in direction:\n",
        "                volumes['North Bound'] = int(volume)\n",
        "            elif 'South' in direction:\n",
        "                volumes['South Bound'] = int(volume)\n",
        "\n",
        "        return list(volumes.values())\n",
        "\n",
        "    def _update_traffic_light(self, action):\n",
        "        self.traffic_light_state = action\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "        if self.current_step < self.num_samples:\n",
        "            total_volume = self.data.iloc[self.current_step]['Total Passing Vehicle Volume']\n",
        "            if self.traffic_light_state == 1:  # Green light\n",
        "                return total_volume / 1000  # Reward based on throughput\n",
        "            else:  # Red light\n",
        "                return -total_volume / 1000  # Penalty for stopping traffic\n",
        "        else:\n",
        "            return 0  # No reward if we're beyond the data"
      ],
      "metadata": {
        "id": "2jdzjeIZVscA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, agent, episodes, max_steps):\n",
        "    scores = []\n",
        "    losses = []\n",
        "    epsilons = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        score = 0\n",
        "        episode_losses = []\n",
        "\n",
        "        for time in range(max_steps):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            score += reward\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # Calculate loss\n",
        "            if len(agent.memory) > BATCH_SIZE:\n",
        "                experiences = agent.memory.sample()\n",
        "                states, actions, rewards, next_states, dones = experiences\n",
        "                Q_targets_next = agent.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "                Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))\n",
        "                Q_expected = agent.qnetwork_local(states).gather(1, actions)\n",
        "                loss = F.mse_loss(Q_expected, Q_targets)\n",
        "                episode_losses.append(loss.item())\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)\n",
        "        scores.append(score)\n",
        "        losses.append(np.mean(episode_losses) if episode_losses else 0)\n",
        "        epsilons.append(agent.epsilon)\n",
        "\n",
        "        if e % 10 == 0:\n",
        "            torch.save(agent.qnetwork_local.state_dict(), f'checkpoint_{e}.pth')\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            print(f\"Episode {e+1}/{episodes}, Score: {score:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
        "\n",
        "    return scores, losses, epsilons"
      ],
      "metadata": {
        "id": "O9VSVmOCV27V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(train_scores, epsilons):\n",
        "    # Create a figure with 1 row and 2 columns for side-by-side plots\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot for training scores\n",
        "    axs[0].plot(train_scores, color='blue')\n",
        "    axs[0].set_title('Training Scores')\n",
        "    axs[0].set_xlabel('Episode')\n",
        "    axs[0].set_ylabel('Score')\n",
        "    axs[0].grid()\n",
        "\n",
        "    # Plot for epsilon decay\n",
        "    axs[1].plot(epsilons, color='orange')\n",
        "    axs[1].set_title('Epsilon Decay')\n",
        "    axs[1].set_xlabel('Episode')\n",
        "    axs[1].set_ylabel('Epsilon')\n",
        "    axs[1].grid()\n",
        "\n",
        "    # Adjust layout and save the figure\n",
        "    plt.tight_layout()  # Automatically adjust subplot parameters for a better fit\n",
        "    plt.savefig('Side_by_Side_Plots.png')  # Save the plot to a file\n",
        "    plt.close()  # Close the plot"
      ],
      "metadata": {
        "id": "4lXp3GXGWE8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load and split the data\n",
        "    train_data = pd.read_csv(\"/content/drive/MyDrive/AAAData Google/average-daily-traffic-counts.csv\")\n",
        "\n",
        "    state_size = 5  # East, West, North, South bound volumes, and Light State\n",
        "    action_size = 2  # 0 = Red, 1 = Green\n",
        "\n",
        "    train_env = TrafficEnv(train_data)\n",
        "\n",
        "    agent = Agent(state_size, action_size, seed=0)\n",
        "\n",
        "    train_episodes = 100\n",
        "    test_episodes = 20\n",
        "    max_steps = 1000\n",
        "\n",
        "    # Training\n",
        "    train_scores, losses, epsilons = train(train_env, agent, train_episodes, max_steps)\n",
        "\n",
        "    # Example usage (make sure train_scores and epsilons are defined before this)\n",
        "    plot_results(train_scores, epsilons)\n",
        "\n",
        "    print(\"Training completed. Results plotted and saved as 'training_results.png'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ7mVOGlWqko",
        "outputId": "f56a3268-bcfc-4db4-950a-de1354c6f9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/100, Score: -677.00, Epsilon: 0.99\n",
            "Episode 6/100, Score: -238.60, Epsilon: 0.97\n",
            "Episode 11/100, Score: 217.60, Epsilon: 0.95\n",
            "Episode 16/100, Score: 1533.00, Epsilon: 0.92\n",
            "Episode 21/100, Score: 1068.40, Epsilon: 0.90\n",
            "Episode 26/100, Score: 2897.00, Epsilon: 0.88\n",
            "Episode 31/100, Score: 1576.80, Epsilon: 0.86\n",
            "Episode 36/100, Score: 4122.00, Epsilon: 0.83\n",
            "Episode 41/100, Score: 2271.60, Epsilon: 0.81\n",
            "Episode 46/100, Score: 3596.40, Epsilon: 0.79\n",
            "Episode 51/100, Score: 4837.20, Epsilon: 0.77\n",
            "Episode 56/100, Score: 4614.80, Epsilon: 0.76\n",
            "Episode 61/100, Score: 4342.00, Epsilon: 0.74\n",
            "Episode 66/100, Score: 4436.00, Epsilon: 0.72\n",
            "Episode 71/100, Score: 5045.00, Epsilon: 0.70\n",
            "Episode 76/100, Score: 5789.60, Epsilon: 0.68\n",
            "Episode 81/100, Score: 5706.60, Epsilon: 0.67\n",
            "Episode 86/100, Score: 6237.20, Epsilon: 0.65\n",
            "Episode 91/100, Score: 6562.60, Epsilon: 0.63\n",
            "Episode 96/100, Score: 7158.60, Epsilon: 0.62\n",
            "Training completed. Results plotted and saved as 'training_results.png'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l78GZxhEWgd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kaaC7l_SWf6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}